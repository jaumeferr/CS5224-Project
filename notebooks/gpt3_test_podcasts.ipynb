{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from pytube import YouTube\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "# import openai\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\NUS\\\\CS5224 Cloud Computing\\\\CS5224-Project\\\\notebooks\\\\test_video.mp4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get video transcription\n",
    "model = whisper.load_model('base')\n",
    "v_url = \"https://www.youtube.com/watch?v=DuaTGng9tRU\"\n",
    "youtube_video = YouTube(v_url) # Select YT video\n",
    "audio_stream_set = youtube_video.streams.filter(only_audio = True)\n",
    "audio_stream = audio_stream_set.first() # Select quality audio stream\n",
    "audio_stream.download(filename = 'test_video.mp4') # Download video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['title', 'index', 'content', 'token']\n",
      "        title  index                                            content  token\n",
      "0  test_video      0   Good morning guys and welcome back to the cha...     76\n",
      "1  test_video      1  I've been back two days and I'm now full of co...    152\n",
      "2  test_video      2  So I've got a pretty busy couple of days ahead...    177\n",
      "3  test_video      3  So a lot of you have been asking for me to do ...    138\n",
      "4  test_video      4                    So that's what I'm going to do.     31\n",
      "5  test_video      5  So first and foremost before the World Cup sta...    393\n",
      "6  test_video      6  Actually if you look at the statistics there w...  10238\n",
      "saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\coolg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Transcribe and save text as csv divided by sentences\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "print(os.path.isfile('test_video.mp4'))\n",
    "path = 'test_video.mp4'\n",
    "t_model = whisper.transcribe(model= model, audio= 'test_video.mp4', fp16 = False) # Get transcript\n",
    "t_model\n",
    "# Tokenize and save as csv file\n",
    "nltk.download('punkt')  # download the NLTK tokenizer\n",
    "transcript = t_model['text']\n",
    "# create a Pandas DataFrame with one row for each sentence\n",
    "trans_df = pd.DataFrame({'content': nltk.sent_tokenize(transcript)})\n",
    "# add a new column with the length of each sentence\n",
    "trans_df['title'] = \"test_video\"\n",
    "trans_df['token'] = trans_df['content'].apply(len)\n",
    "trans_df = trans_df.reset_index()\n",
    "trans_df = trans_df[['title', 'index', 'content', 'token']]\n",
    "print(list(trans_df.columns.values))\n",
    "\n",
    "# print the DataFrame\n",
    "print(trans_df)\n",
    "# save the DataFrame to a CSV file\n",
    "trans_df.to_csv('video_text.csv', index=False)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 rows in the data.\n",
      "        title  heading                                            content  \\\n",
      "0  test_video        0   Good morning guys and welcome back to the cha...   \n",
      "1  test_video        1  I've been back two days and I'm now full of co...   \n",
      "2  test_video        2  So I've got a pretty busy couple of days ahead...   \n",
      "3  test_video        3  So a lot of you have been asking for me to do ...   \n",
      "4  test_video        4                    So that's what I'm going to do.   \n",
      "5  test_video        5  So first and foremost before the World Cup sta...   \n",
      "6  test_video        6  Actually if you look at the statistics there w...   \n",
      "\n",
      "   token  \n",
      "0     76  \n",
      "1    152  \n",
      "2    177  \n",
      "3    138  \n",
      "4     31  \n",
      "5    393  \n",
      "6  10238  \n"
     ]
    }
   ],
   "source": [
    "# help(pd.read_csv)\n",
    "df = pd.read_csv('video_text.csv', header=0, names=[\"title\", \"heading\", \"content\", \"token\"])\n",
    "print(f\"{len(df)} rows in the data.\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the transcript and query embeddings\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "prompt = \"Why should you start being selfish?\"\n",
    "\n",
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }\n",
    "\n",
    "def load_embeddings(fname: str) -> dict[tuple[str, str], list[float]]:\n",
    "    \"\"\"\n",
    "    Read the document embeddings and their keys from a CSV.\n",
    "    \n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"title\", \"heading\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\"\n",
    "test_emb = get_embedding(prompt, EMBEDDING_MODEL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** RESULT ***\n",
    "\n",
    "{\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"embedding\": [\n",
    "        -0.006929283495992422,\n",
    "        -0.005336422007530928,\n",
    "        ...\n",
    "        -4.547132266452536e-05,\n",
    "        -0.024047505110502243\n",
    "      ],\n",
    "      \"index\": 0,\n",
    "      \"object\": \"embedding\"\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"text-embedding-ada-002\",\n",
    "  \"object\": \"list\",\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 5,\n",
    "    \"total_tokens\": 5\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a9cbf84dd4a29b46cdff894ec5e0ef4cc44a632636ec2c548fb4524f05c5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
